{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9CCif1NWGVK"
      },
      "source": [
        "# Module 2: SLURM and Spark on SDSC Expanse\n",
        "\n",
        "**DSC 232R - Big Data Analysis Using Spark**\n",
        "\n",
        "This notebook covers:\n",
        "1. Understanding SLURM job scheduling\n",
        "2. Writing effective SLURM scripts\n",
        "3. Configuring Spark for HPC environments\n",
        "4. Using Singularity containers\n",
        "\n",
        "**Note**: This notebook is designed for learning concepts locally. The SLURM commands shown are for reference - actual execution requires an HPC cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeCGUcSoWGVO"
      },
      "source": [
        "## Key Takeaways\n",
        "\n",
        "- **SLURM** manages resources and schedules jobs on HPC clusters\n",
        "- **sbatch** submits batch jobs; **srun** runs interactive/parallel commands\n",
        "- **Spark configuration** must match your SLURM resource allocation\n",
        "- **Singularity containers** ensure reproducible environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZi1BETPWGVO"
      },
      "source": [
        "---\n",
        "\n",
        "## 1. SLURM Basics\n",
        "\n",
        "### What is SLURM?\n",
        "\n",
        "SLURM (Simple Linux Utility for Resource Management) is a job scheduler that:\n",
        "- Allocates compute resources (nodes, CPUs, memory, GPUs)\n",
        "- Manages job queues with priorities\n",
        "- Tracks usage for billing/allocation purposes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGBniHGAWGVO",
        "outputId": "9e613708-a9b9-46ce-cf76-e1c53ed2672f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SLURM Command Reference\n",
            "==================================================\n",
            "sbatch       - Submit a batch job script\n",
            "srun         - Run a command within an allocation\n",
            "squeue       - View job queue status\n",
            "scancel      - Cancel a job\n",
            "sinfo        - View partition/node information\n",
            "sacct        - View job accounting data\n",
            "seff         - View job efficiency metrics\n"
          ]
        }
      ],
      "source": [
        "# SLURM Command Reference (for use on SDSC)\n",
        "# These commands won't work locally - they're here for documentation\n",
        "\n",
        "slurm_commands = {\n",
        "    'sbatch': 'Submit a batch job script',\n",
        "    'srun': 'Run a command within an allocation',\n",
        "    'squeue': 'View job queue status',\n",
        "    'scancel': 'Cancel a job',\n",
        "    'sinfo': 'View partition/node information',\n",
        "    'sacct': 'View job accounting data',\n",
        "    'seff': 'View job efficiency metrics'\n",
        "}\n",
        "\n",
        "print(\"SLURM Command Reference\")\n",
        "print(\"=\" * 50)\n",
        "for cmd, desc in slurm_commands.items():\n",
        "    print(f\"{cmd:12} - {desc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxzcWZh-WGVQ"
      },
      "source": [
        "### Expanse Partitions\n",
        "\n",
        "Different partitions have different resource limits and queue times:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ru-HDQuWGVQ",
        "outputId": "bdcaf427-5a91-4ece-b6da-5871bead42b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SDSC Expanse Partitions\n",
            "================================================================================\n",
            " Partition  Max Nodes Max Time  Max Cores/Node                Use Case\n",
            "     debug          2   30 min             128    Testing, development\n",
            "    shared          1   48 hrs             128 Small jobs (<128 cores)\n",
            "   compute         32   48 hrs             128     Large parallel jobs\n",
            "       gpu          4   48 hrs              40          Multi-GPU jobs\n",
            "gpu-shared          1   48 hrs              10         Single GPU jobs\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "partitions = pd.DataFrame({\n",
        "    'Partition': ['debug', 'shared', 'compute', 'gpu', 'gpu-shared'],\n",
        "    'Max Nodes': [2, 1, 32, 4, 1],\n",
        "    'Max Time': ['30 min', '48 hrs', '48 hrs', '48 hrs', '48 hrs'],\n",
        "    'Max Cores/Node': [128, 128, 128, 40, 10],\n",
        "    'Use Case': [\n",
        "        'Testing, development',\n",
        "        'Small jobs (<128 cores)',\n",
        "        'Large parallel jobs',\n",
        "        'Multi-GPU jobs',\n",
        "        'Single GPU jobs'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"SDSC Expanse Partitions\")\n",
        "print(\"=\" * 80)\n",
        "print(partitions.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAcZ3qapWGVQ"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Anatomy of a SLURM Script\n",
        "\n",
        "Let's break down a SLURM job script piece by piece:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY8Imv8tWGVR",
        "outputId": "e726ec83-aca9-4f30-b0fb-a229fd2a664e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example SLURM Script\n",
            "============================================================\n",
            "\n",
            "#!/bin/bash\n",
            "#SBATCH --job-name=my_spark_job      # Name shown in queue\n",
            "#SBATCH --partition=shared           # Which queue to use\n",
            "#SBATCH --nodes=1                    # Number of nodes\n",
            "#SBATCH --ntasks-per-node=1          # MPI tasks (usually 1 for Spark)\n",
            "#SBATCH --cpus-per-task=32           # CPU cores per task\n",
            "#SBATCH --mem=128G                   # Memory per node\n",
            "#SBATCH --time=04:00:00              # Wall clock limit (HH:MM:SS)\n",
            "#SBATCH --account=uci150             # Allocation to charge\n",
            "#SBATCH --output=logs/job_%j.out     # Standard output (%j = job ID)\n",
            "#SBATCH --error=logs/job_%j.err      # Standard error\n",
            "\n",
            "# Load required modules\n",
            "module load singularitypro\n",
            "\n",
            "# Run the job\n",
            "singularity exec container.sif python script.py\n",
            "\n"
          ]
        }
      ],
      "source": [
        "slurm_script = '''\n",
        "#!/bin/bash\n",
        "#SBATCH --job-name=my_spark_job      # Name shown in queue\n",
        "#SBATCH --partition=shared           # Which queue to use\n",
        "#SBATCH --nodes=1                    # Number of nodes\n",
        "#SBATCH --ntasks-per-node=1          # MPI tasks (usually 1 for Spark)\n",
        "#SBATCH --cpus-per-task=32           # CPU cores per task\n",
        "#SBATCH --mem=128G                   # Memory per node\n",
        "#SBATCH --time=04:00:00              # Wall clock limit (HH:MM:SS)\n",
        "#SBATCH --account=uci150             # Allocation to charge\n",
        "#SBATCH --output=logs/job_%j.out     # Standard output (%j = job ID)\n",
        "#SBATCH --error=logs/job_%j.err      # Standard error\n",
        "\n",
        "# Load required modules\n",
        "module load singularitypro\n",
        "\n",
        "# Run the job\n",
        "singularity exec container.sif python script.py\n",
        "'''\n",
        "\n",
        "print(\"Example SLURM Script\")\n",
        "print(\"=\" * 60)\n",
        "print(slurm_script)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T4uqnqXWGVR"
      },
      "source": [
        "### Understanding SBATCH Options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aL2pOSWWGVR",
        "outputId": "1bfd1c65-d060-419f-d054-82ced58e6c11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SBATCH Options Reference\n",
            "================================================================================\n",
            "         Option              Description              Example\n",
            "    --partition     Queue/partition name shared, compute, gpu\n",
            "        --nodes  Number of compute nodes             1, 4, 32\n",
            "       --ntasks          Total MPI tasks 1 (for Python/Spark)\n",
            "--cpus-per-task       CPU cores per task       8, 32, 64, 128\n",
            "          --mem          Memory per node      32G, 128G, 256G\n",
            "         --time    Wall clock time limit   01:00:00, 48:00:00\n",
            "         --gres Generic resources (GPUs)         gpu:1, gpu:4\n",
            "      --account       Allocation account               uci150\n"
          ]
        }
      ],
      "source": [
        "sbatch_options = pd.DataFrame({\n",
        "    'Option': [\n",
        "        '--partition', '--nodes', '--ntasks', '--cpus-per-task',\n",
        "        '--mem', '--time', '--gres', '--account'\n",
        "    ],\n",
        "    'Description': [\n",
        "        'Queue/partition name',\n",
        "        'Number of compute nodes',\n",
        "        'Total MPI tasks',\n",
        "        'CPU cores per task',\n",
        "        'Memory per node',\n",
        "        'Wall clock time limit',\n",
        "        'Generic resources (GPUs)',\n",
        "        'Allocation account'\n",
        "    ],\n",
        "    'Example': [\n",
        "        'shared, compute, gpu',\n",
        "        '1, 4, 32',\n",
        "        '1 (for Python/Spark)',\n",
        "        '8, 32, 64, 128',\n",
        "        '32G, 128G, 256G',\n",
        "        '01:00:00, 48:00:00',\n",
        "        'gpu:1, gpu:4',\n",
        "        'uci150'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"SBATCH Options Reference\")\n",
        "print(\"=\" * 80)\n",
        "print(sbatch_options.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-9aLj3HWGVS"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Spark Configuration for HPC\n",
        "\n",
        "### Memory Layout\n",
        "\n",
        "When running Spark on HPC, you need to carefully allocate memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiabvhXCWGVS",
        "outputId": "f930a7cb-b74a-4a81-a1da-83fb98a8872d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Configuration Examples\n",
            "============================================================\n",
            "\n",
            "32 GB Memory, 8 Cores:\n",
            "  spark.driver.memory: 4g\n",
            "  spark.executor.memory: 26g\n",
            "  spark.sql.shuffle.partitions: 16\n",
            "  spark.default.parallelism: 16\n",
            "  spark.driver.maxResultSize: 4g\n",
            "\n",
            "128 GB Memory, 32 Cores:\n",
            "  spark.driver.memory: 15g\n",
            "  spark.executor.memory: 103g\n",
            "  spark.sql.shuffle.partitions: 64\n",
            "  spark.default.parallelism: 64\n",
            "  spark.driver.maxResultSize: 4g\n",
            "\n",
            "256 GB Memory, 64 Cores:\n",
            "  spark.driver.memory: 30g\n",
            "  spark.executor.memory: 206g\n",
            "  spark.sql.shuffle.partitions: 128\n",
            "  spark.default.parallelism: 128\n",
            "  spark.driver.maxResultSize: 4g\n"
          ]
        }
      ],
      "source": [
        "def calculate_spark_config(total_memory_gb, num_cores):\n",
        "    \"\"\"\n",
        "    Calculate optimal Spark configuration based on SLURM allocation.\n",
        "\n",
        "    Guidelines:\n",
        "    - Driver: ~10-15% of total memory\n",
        "    - Executor: ~75-80% of total memory\n",
        "    - Overhead: ~10% for OS and Spark internals\n",
        "    \"\"\"\n",
        "    driver_mem = max(4, int(total_memory_gb * 0.12))  # 12% for driver\n",
        "    overhead = max(2, int(total_memory_gb * 0.08))    # 8% overhead\n",
        "    executor_mem = total_memory_gb - driver_mem - overhead\n",
        "\n",
        "    # Shuffle partitions: 2-3x number of cores\n",
        "    shuffle_partitions = num_cores * 2\n",
        "\n",
        "    return {\n",
        "        'spark.driver.memory': f'{driver_mem}g',\n",
        "        'spark.executor.memory': f'{executor_mem}g',\n",
        "        'spark.sql.shuffle.partitions': shuffle_partitions,\n",
        "        'spark.default.parallelism': shuffle_partitions,\n",
        "        'spark.driver.maxResultSize': '4g'\n",
        "    }\n",
        "\n",
        "# Example configurations\n",
        "configs = [\n",
        "    (32, 8),    # Small job: 32 GB, 8 cores\n",
        "    (128, 32),  # Medium job: 128 GB, 32 cores\n",
        "    (256, 64),  # Large job: 256 GB, 64 cores\n",
        "]\n",
        "\n",
        "print(\"Spark Configuration Examples\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for mem, cores in configs:\n",
        "    config = calculate_spark_config(mem, cores)\n",
        "    print(f\"\\n{mem} GB Memory, {cores} Cores:\")\n",
        "    for key, value in config.items():\n",
        "        print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTLOX0gBWGVS"
      },
      "source": [
        "### SparkSession Configuration Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDOdPIW5WGVS",
        "outputId": "fe7b87aa-81f8-49eb-f3e5-5e575be65375",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkSession Template for SLURM\n",
            "============================================================\n",
            "\n",
            "from pyspark.sql import SparkSession\n",
            "import os\n",
            "\n",
            "def create_spark_session_for_slurm():\n",
            "    \"\"\"\n",
            "    Create SparkSession configured for SLURM environment.\n",
            "    Reads resource allocation from SLURM environment variables.\n",
            "    \"\"\"\n",
            "    # Get SLURM allocation\n",
            "    cpus = int(os.environ.get('SLURM_CPUS_PER_TASK', 8))\n",
            "    mem_str = os.environ.get('SLURM_MEM_PER_NODE', '32G')\n",
            "    \n",
            "    # Parse memory (handle both '32G' and '32000' formats)\n",
            "    if 'G' in mem_str:\n",
            "        mem_gb = int(mem_str.replace('G', ''))\n",
            "    else:\n",
            "        mem_gb = int(mem_str) // 1024\n",
            "    \n",
            "    # Calculate allocations\n",
            "    driver_mem = max(4, mem_gb // 8)\n",
            "    executor_mem = mem_gb - driver_mem - 2\n",
            "    \n",
            "    spark = SparkSession.builder \\\n",
            "        .appName(os.environ.get('SLURM_JOB_NAME', 'SparkJob')) \\\n",
            "        .master(f'local[{cpus}]') \\\n",
            "        .config('spark.driver.memory', f'{driver_mem}g') \\\n",
            "        .config('spark.executor.memory', f'{executor_mem}g') \\\n",
            "        .config('spark.driver.maxResultSize', '4g') \\\n",
            "        .config('spark.sql.shuffle.partitions', str(cpus * 2)) \\\n",
            "        .config('spark.default.parallelism', str(cpus * 2)) \\\n",
            "        .getOrCreate()\n",
            "    \n",
            "    return spark\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark_template = '''\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "def create_spark_session_for_slurm():\n",
        "    \"\"\"\n",
        "    Create SparkSession configured for SLURM environment.\n",
        "    Reads resource allocation from SLURM environment variables.\n",
        "    \"\"\"\n",
        "    # Get SLURM allocation\n",
        "    cpus = int(os.environ.get('SLURM_CPUS_PER_TASK', 8))\n",
        "    mem_str = os.environ.get('SLURM_MEM_PER_NODE', '32G')\n",
        "\n",
        "    # Parse memory (handle both '32G' and '32000' formats)\n",
        "    if 'G' in mem_str:\n",
        "        mem_gb = int(mem_str.replace('G', ''))\n",
        "    else:\n",
        "        mem_gb = int(mem_str) // 1024\n",
        "\n",
        "    # Calculate allocations\n",
        "    driver_mem = max(4, mem_gb // 8)\n",
        "    executor_mem = mem_gb - driver_mem - 2\n",
        "\n",
        "    spark = SparkSession.builder \\\\\n",
        "        .appName(os.environ.get('SLURM_JOB_NAME', 'SparkJob')) \\\\\n",
        "        .master(f'local[{cpus}]') \\\\\n",
        "        .config('spark.driver.memory', f'{driver_mem}g') \\\\\n",
        "        .config('spark.executor.memory', f'{executor_mem}g') \\\\\n",
        "        .config('spark.driver.maxResultSize', '4g') \\\\\n",
        "        .config('spark.sql.shuffle.partitions', str(cpus * 2)) \\\\\n",
        "        .config('spark.default.parallelism', str(cpus * 2)) \\\\\n",
        "        .getOrCreate()\n",
        "\n",
        "    return spark\n",
        "'''\n",
        "\n",
        "print(\"SparkSession Template for SLURM\")\n",
        "print(\"=\" * 60)\n",
        "print(spark_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB9YopVhWGVT"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Local Simulation: Understanding Resource Allocation\n",
        "\n",
        "Let's simulate how Spark uses resources locally:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSUX2Zj-WGVT",
        "outputId": "079e88ea-ed9e-4875-a178-dd91b20bb366",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local System Resources\n",
            "========================================\n",
            "CPUs: 2\n",
            "Memory: 12.7 GB\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "# Get local system resources\n",
        "local_cpus = os.cpu_count()\n",
        "local_memory_gb = psutil.virtual_memory().total / (1024**3)\n",
        "\n",
        "print(f\"Local System Resources\")\n",
        "print(f\"=\" * 40)\n",
        "print(f\"CPUs: {local_cpus}\")\n",
        "print(f\"Memory: {local_memory_gb:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSXwsy2ZWGVT",
        "outputId": "4c6ee76a-3bef-4118-c414-64c3fd44a7f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulated SLURM Allocation\n",
            "========================================\n",
            "CPUs: 2\n",
            "Memory: 3 GB\n",
            "\n",
            "SparkSession created:\n",
            "  App Name: SLURM_Simulation\n",
            "  Master: local[2]\n",
            "  Default Parallelism: 2\n"
          ]
        }
      ],
      "source": [
        "# Create SparkSession with limited resources (simulating shared node)\n",
        "# Limit to half the local resources to avoid impacting system\n",
        "\n",
        "sim_cpus = max(2, local_cpus // 2)\n",
        "sim_memory = max(2, int(local_memory_gb * 0.3))\n",
        "\n",
        "print(f\"Simulated SLURM Allocation\")\n",
        "print(f\"=\" * 40)\n",
        "print(f\"CPUs: {sim_cpus}\")\n",
        "print(f\"Memory: {sim_memory} GB\")\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SLURM_Simulation\") \\\n",
        "    .master(f\"local[{sim_cpus}]\") \\\n",
        "    .config(\"spark.driver.memory\", f\"{max(1, sim_memory // 4)}g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", str(sim_cpus * 2)) \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"\\nSparkSession created:\")\n",
        "print(f\"  App Name: {spark.sparkContext.appName}\")\n",
        "print(f\"  Master: {spark.sparkContext.master}\")\n",
        "print(f\"  Default Parallelism: {spark.sparkContext.defaultParallelism}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekMxrvRRWGVT",
        "outputId": "1e43415f-23a0-4862-87be-76cfab03a6b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Partition Performance Test (1,000,000 elements)\n",
            "==================================================\n",
            "  Partitions     Time (s)   Per Partition\n",
            "--------------------------------------------------\n",
            "           2        3.046         500,000\n",
            "           4        1.405         250,000\n",
            "           8        1.909         125,000\n",
            "          16        3.950          62,500\n"
          ]
        }
      ],
      "source": [
        "# Demonstrate parallelism with different partition counts\n",
        "import numpy as np\n",
        "\n",
        "# Generate test data\n",
        "n_elements = 1_000_000\n",
        "data = list(range(n_elements))\n",
        "\n",
        "def measure_execution_time(num_partitions):\n",
        "    \"\"\"Measure time to process data with given partition count.\"\"\"\n",
        "    rdd = spark.sparkContext.parallelize(data, num_partitions)\n",
        "\n",
        "    start = time.time()\n",
        "    result = rdd.map(lambda x: x * 2).reduce(lambda a, b: a + b)\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    return elapsed, result\n",
        "\n",
        "# Test different partition counts\n",
        "partition_counts = [sim_cpus, sim_cpus * 2, sim_cpus * 4, sim_cpus * 8]\n",
        "\n",
        "print(f\"\\nPartition Performance Test ({n_elements:,} elements)\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"{'Partitions':>12} {'Time (s)':>12} {'Per Partition':>15}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for num_parts in partition_counts:\n",
        "    elapsed, result = measure_execution_time(num_parts)\n",
        "    elements_per_partition = n_elements // num_parts\n",
        "    print(f\"{num_parts:>12} {elapsed:>12.3f} {elements_per_partition:>15,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhjScx5aWGVT"
      },
      "source": [
        "### Partition Count Guidelines\n",
        "\n",
        "The optimal number of partitions depends on:\n",
        "1. **Number of cores** - Each partition can run on one core\n",
        "2. **Data size** - Each partition should have enough data to process efficiently\n",
        "3. **Memory per core** - Partitions must fit in available memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJKEYrnHWGVT",
        "outputId": "0a34eef8-1ac3-41dd-a907-b337c02431e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partition Recommendations\n",
            "======================================================================\n",
            "\n",
            "Small job: 10 GB data, 8 cores, 4 GB/core\n",
            "  Recommended partitions: 66\n",
            "  (by size: 66, by cores: 16, max by memory: 106\n",
            "\n",
            "Medium job: 50 GB data, 32 cores, 4 GB/core\n",
            "  Recommended partitions: 333\n",
            "  (by size: 333, by cores: 64, max by memory: 426\n",
            "\n",
            "Large job: 200 GB data, 64 cores, 4 GB/core\n",
            "  Recommended partitions: 853\n",
            "  (by size: 1333, by cores: 128, max by memory: 853\n"
          ]
        }
      ],
      "source": [
        "def recommend_partitions(data_size_gb, memory_per_core_gb, num_cores):\n",
        "    \"\"\"\n",
        "    Recommend partition count based on data and resources.\n",
        "\n",
        "    Rules of thumb:\n",
        "    - Target 100-200 MB per partition for typical workloads\n",
        "    - At least 2x partitions per core for parallelism\n",
        "    - Don't exceed memory capacity\n",
        "    \"\"\"\n",
        "    target_partition_size_gb = 0.15  # 150 MB target\n",
        "\n",
        "    # Based on data size\n",
        "    partitions_by_size = max(1, int(data_size_gb / target_partition_size_gb))\n",
        "\n",
        "    # Based on parallelism (2-3x cores)\n",
        "    partitions_by_cores = num_cores * 2\n",
        "\n",
        "    # Based on memory (each partition needs ~2x its size in memory)\n",
        "    max_partitions_by_memory = int((memory_per_core_gb * num_cores) / (target_partition_size_gb * 2))\n",
        "\n",
        "    recommended = min(partitions_by_size, max_partitions_by_memory)\n",
        "    recommended = max(recommended, partitions_by_cores)  # At least 2x cores\n",
        "\n",
        "    return {\n",
        "        'recommended': recommended,\n",
        "        'by_data_size': partitions_by_size,\n",
        "        'by_core_count': partitions_by_cores,\n",
        "        'max_by_memory': max_partitions_by_memory\n",
        "    }\n",
        "\n",
        "# Example scenarios\n",
        "scenarios = [\n",
        "    ('Small job', 10, 4, 8),     # 10 GB data, 4 GB/core, 8 cores\n",
        "    ('Medium job', 50, 4, 32),   # 50 GB data, 4 GB/core, 32 cores\n",
        "    ('Large job', 200, 4, 64),   # 200 GB data, 4 GB/core, 64 cores\n",
        "]\n",
        "\n",
        "print(\"Partition Recommendations\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for name, data_gb, mem_per_core, cores in scenarios:\n",
        "    rec = recommend_partitions(data_gb, mem_per_core, cores)\n",
        "    print(f\"\\n{name}: {data_gb} GB data, {cores} cores, {mem_per_core} GB/core\")\n",
        "    print(f\"  Recommended partitions: {rec['recommended']}\")\n",
        "    print(f\"  (by size: {rec['by_data_size']}, by cores: {rec['by_core_count']}, max by memory: {rec['max_by_memory']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_85dVvZWGVU"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. Singularity Container Usage\n",
        "\n",
        "Singularity containers ensure consistent environments across different systems:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHo1SgY-WGVU",
        "outputId": "ccf9822b-f0fe-4c94-fa7e-afec831bbe02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# Singularity Command Reference\n",
            "# ==============================\n",
            "\n",
            "# 1. Build a container (requires sudo, do locally)\n",
            "sudo singularity build container.sif Singularity.def\n",
            "\n",
            "# 2. Transfer to SDSC\n",
            "scp container.sif user@login.expanse.sdsc.edu:/expanse/lustre/projects/uci150/user/\n",
            "\n",
            "# 3. Interactive shell in container\n",
            "singularity shell container.sif\n",
            "\n",
            "# 4. Execute a command in container\n",
            "singularity exec container.sif python --version\n",
            "\n",
            "# 5. Bind external paths (access host filesystem)\n",
            "singularity exec --bind /expanse container.sif python script.py\n",
            "\n",
            "# 6. Multiple bind points\n",
            "singularity exec \\\n",
            "    --bind /expanse/lustre/projects/uci150 \\\n",
            "    --bind /scratch/$USER \\\n",
            "    container.sif python script.py\n",
            "\n",
            "# 7. Set environment variables\n",
            "singularity exec --env PYTHONPATH=/app container.sif python script.py\n",
            "\n"
          ]
        }
      ],
      "source": [
        "singularity_commands = '''\n",
        "# Singularity Command Reference\n",
        "# ==============================\n",
        "\n",
        "# 1. Build a container (requires sudo, do locally)\n",
        "sudo singularity build container.sif Singularity.def\n",
        "\n",
        "# 2. Transfer to SDSC\n",
        "scp container.sif user@login.expanse.sdsc.edu:/expanse/lustre/projects/uci150/user/\n",
        "\n",
        "# 3. Interactive shell in container\n",
        "singularity shell container.sif\n",
        "\n",
        "# 4. Execute a command in container\n",
        "singularity exec container.sif python --version\n",
        "\n",
        "# 5. Bind external paths (access host filesystem)\n",
        "singularity exec --bind /expanse container.sif python script.py\n",
        "\n",
        "# 6. Multiple bind points\n",
        "singularity exec \\\\\n",
        "    --bind /expanse/lustre/projects/uci150 \\\\\n",
        "    --bind /scratch/$USER \\\\\n",
        "    container.sif python script.py\n",
        "\n",
        "# 7. Set environment variables\n",
        "singularity exec --env PYTHONPATH=/app container.sif python script.py\n",
        "'''\n",
        "\n",
        "print(singularity_commands)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hWF9U5KWGVU"
      },
      "source": [
        "### Container Definition File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXqWrH-FWGVU",
        "outputId": "b334a822-6e65-4636-bcaf-f37b0c7919bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Singularity Definition\n",
            "==================================================\n",
            "\n",
            "Bootstrap: docker\n",
            "From: python:3.10-slim\n",
            "\n",
            "%labels\n",
            "    Author DSC 232R\n",
            "    Description Spark + Ray container for HPC\n",
            "\n",
            "%post\n",
            "    # System packages\n",
            "    apt-get update && apt-get install -y \\\n",
            "        openjdk-17-jdk-headless \\\n",
            "        wget curl git \\\n",
            "        && rm -rf /var/lib/apt/lists/*\n",
            "\n",
            "    # Python packages\n",
            "    pip install --no-cache-dir \\\n",
            "        pyspark>=3.5.0 \\\n",
            "        numpy pandas matplotlib \\\n",
            "        scikit-learn xgboost \\\n",
            "        jupyter\n",
            "\n",
            "%environment\n",
            "    export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\n",
            "    export PATH=$JAVA_HOME/bin:$PATH\n",
            "\n",
            "%runscript\n",
            "    python \"$@\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "singularity_def = '''\n",
        "Bootstrap: docker\n",
        "From: python:3.10-slim\n",
        "\n",
        "%labels\n",
        "    Author DSC 232R\n",
        "    Description Spark + Ray container for HPC\n",
        "\n",
        "%post\n",
        "    # System packages\n",
        "    apt-get update && apt-get install -y \\\\\n",
        "        openjdk-17-jdk-headless \\\\\n",
        "        wget curl git \\\\\n",
        "        && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "    # Python packages\n",
        "    pip install --no-cache-dir \\\\\n",
        "        pyspark>=3.5.0 \\\\\n",
        "        numpy pandas matplotlib \\\\\n",
        "        scikit-learn xgboost \\\\\n",
        "        jupyter\n",
        "\n",
        "%environment\n",
        "    export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\n",
        "    export PATH=$JAVA_HOME/bin:$PATH\n",
        "\n",
        "%runscript\n",
        "    python \"$@\"\n",
        "'''\n",
        "\n",
        "print(\"Example Singularity Definition\")\n",
        "print(\"=\" * 50)\n",
        "print(singularity_def)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciZiRb2gWGVU"
      },
      "source": [
        "---\n",
        "\n",
        "## 6. Exercise: Write a SLURM Script\n",
        "\n",
        "Create a SLURM script for the following scenario:\n",
        "\n",
        "**Requirements:**\n",
        "- Process 50 GB of weather data\n",
        "- Need ~200 GB memory for processing overhead\n",
        "- Want to use 64 cores\n",
        "- Expected runtime: 2 hours\n",
        "- Use the shared partition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ViAZsLdWGVU",
        "outputId": "d11e3779-f39a-4260-af60-e9c3b6582979",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fill in the blanks in the script above!\n",
            "\n",
            "Hints:\n",
            "- 'shared' partition allows up to 128 cores per node\n",
            "- Max memory on shared is 256 GB per node\n",
            "- Time format is HH:MM:SS\n"
          ]
        }
      ],
      "source": [
        "# Exercise: Fill in the SLURM script template\n",
        "\n",
        "# Think about:\n",
        "# 1. Which partition supports 64 cores on one node?\n",
        "# 2. How much memory can you request on shared partition?\n",
        "# 3. What's the appropriate time format?\n",
        "\n",
        "your_script = '''\n",
        "#!/bin/bash\n",
        "#SBATCH --job-name=weather_analysis\n",
        "#SBATCH --partition=???           # Which partition?\n",
        "#SBATCH --nodes=???\n",
        "#SBATCH --ntasks-per-node=???\n",
        "#SBATCH --cpus-per-task=???       # 64 cores\n",
        "#SBATCH --mem=???                 # ~200 GB needed\n",
        "#SBATCH --time=???                # 2 hours\n",
        "#SBATCH --account=uci150\n",
        "#SBATCH --output=logs/weather_%j.out\n",
        "#SBATCH --error=logs/weather_%j.err\n",
        "\n",
        "module load singularitypro\n",
        "\n",
        "singularity exec \\\\\n",
        "    --bind /expanse \\\\\n",
        "    ray_spark_dsc232r.sif \\\\\n",
        "    python weather_analysis.py\n",
        "'''\n",
        "\n",
        "print(\"Fill in the blanks in the script above!\")\n",
        "print(\"\\nHints:\")\n",
        "print(\"- 'shared' partition allows up to 128 cores per node\")\n",
        "print(\"- Max memory on shared is 256 GB per node\")\n",
        "print(\"- Time format is HH:MM:SS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dt21xO5-WGVU",
        "outputId": "b01c59a0-1271-4780-b6e6-31360085b265",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution:\n",
            "==================================================\n",
            "\n",
            "#!/bin/bash\n",
            "#SBATCH --job-name=weather_analysis\n",
            "#SBATCH --partition=shared         # Shared supports up to 128 cores/node\n",
            "#SBATCH --nodes=1                  # Single node is sufficient\n",
            "#SBATCH --ntasks-per-node=1        # One task for Spark driver\n",
            "#SBATCH --cpus-per-task=64         # 64 cores requested\n",
            "#SBATCH --mem=200G                 # 200 GB as specified\n",
            "#SBATCH --time=02:00:00            # 2 hours (HH:MM:SS format)\n",
            "#SBATCH --account=uci150\n",
            "#SBATCH --output=logs/weather_%j.out\n",
            "#SBATCH --error=logs/weather_%j.err\n",
            "\n",
            "module load singularitypro\n",
            "\n",
            "singularity exec \\\n",
            "    --bind /expanse \\\n",
            "    ray_spark_dsc232r.sif \\\n",
            "    python weather_analysis.py\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Solution\n",
        "\n",
        "solution_script = '''\n",
        "#!/bin/bash\n",
        "#SBATCH --job-name=weather_analysis\n",
        "#SBATCH --partition=shared         # Shared supports up to 128 cores/node\n",
        "#SBATCH --nodes=1                  # Single node is sufficient\n",
        "#SBATCH --ntasks-per-node=1        # One task for Spark driver\n",
        "#SBATCH --cpus-per-task=64         # 64 cores requested\n",
        "#SBATCH --mem=200G                 # 200 GB as specified\n",
        "#SBATCH --time=02:00:00            # 2 hours (HH:MM:SS format)\n",
        "#SBATCH --account=uci150\n",
        "#SBATCH --output=logs/weather_%j.out\n",
        "#SBATCH --error=logs/weather_%j.err\n",
        "\n",
        "module load singularitypro\n",
        "\n",
        "singularity exec \\\\\n",
        "    --bind /expanse \\\\\n",
        "    ray_spark_dsc232r.sif \\\\\n",
        "    python weather_analysis.py\n",
        "'''\n",
        "\n",
        "print(\"Solution:\")\n",
        "print(\"=\" * 50)\n",
        "print(solution_script)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWgsKgxEWGVU"
      },
      "source": [
        "---\n",
        "\n",
        "## 7. Job Monitoring Reference\n",
        "\n",
        "Commands to monitor and debug jobs on SDSC:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFA3wILVWGVU",
        "outputId": "408936fa-dbd3-477c-cc3e-234cfb7fb557",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# SLURM Job Monitoring Commands\n",
            "# =============================\n",
            "\n",
            "# View your jobs in the queue\n",
            "squeue -u $USER\n",
            "\n",
            "# Detailed job information\n",
            "scontrol show job <job_id>\n",
            "\n",
            "# View output in real-time\n",
            "tail -f logs/job_12345.out\n",
            "\n",
            "# Check job efficiency after completion\n",
            "seff <job_id>\n",
            "\n",
            "# View detailed accounting\n",
            "sacct -j <job_id> --format=JobID,JobName,Partition,State,ExitCode,Elapsed,MaxRSS,MaxVMSize\n",
            "\n",
            "# Cancel a job\n",
            "scancel <job_id>\n",
            "\n",
            "# Cancel all your jobs\n",
            "scancel -u $USER\n",
            "\n",
            "# View partition information\n",
            "sinfo -p shared\n",
            "\n",
            "# View your allocation balance\n",
            "expanse-client user\n",
            "\n"
          ]
        }
      ],
      "source": [
        "monitoring_commands = '''\n",
        "# SLURM Job Monitoring Commands\n",
        "# =============================\n",
        "\n",
        "# View your jobs in the queue\n",
        "squeue -u $USER\n",
        "\n",
        "# Detailed job information\n",
        "scontrol show job <job_id>\n",
        "\n",
        "# View output in real-time\n",
        "tail -f logs/job_12345.out\n",
        "\n",
        "# Check job efficiency after completion\n",
        "seff <job_id>\n",
        "\n",
        "# View detailed accounting\n",
        "sacct -j <job_id> --format=JobID,JobName,Partition,State,ExitCode,Elapsed,MaxRSS,MaxVMSize\n",
        "\n",
        "# Cancel a job\n",
        "scancel <job_id>\n",
        "\n",
        "# Cancel all your jobs\n",
        "scancel -u $USER\n",
        "\n",
        "# View partition information\n",
        "sinfo -p shared\n",
        "\n",
        "# View your allocation balance\n",
        "expanse-client user\n",
        "'''\n",
        "\n",
        "print(monitoring_commands)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnxs_i-4WGVV"
      },
      "source": [
        "### Common Issues and Solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--GhzQLdWGVV",
        "outputId": "c27c25c1-bc55-417f-bf63-e2614d4c726f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Troubleshooting Guide\n",
            "==========================================================================================\n",
            "                Issue                         Cause                                Solution\n",
            "Job killed (exit 137)           Out of memory (OOM)         Increase --mem or optimize code\n",
            "          Job timeout           Exceeded time limit     Increase --time or parallelize more\n",
            "  ModuleNotFoundError      Package not in container          Rebuild container with package\n",
            "    FileNotFoundError Path not bound in Singularity    Add --bind /path to singularity exec\n",
            "  Job PENDING forever       Resources not available Try debug partition or reduce resources\n"
          ]
        }
      ],
      "source": [
        "troubleshooting = pd.DataFrame({\n",
        "    'Issue': [\n",
        "        'Job killed (exit 137)',\n",
        "        'Job timeout',\n",
        "        'ModuleNotFoundError',\n",
        "        'FileNotFoundError',\n",
        "        'Job PENDING forever'\n",
        "    ],\n",
        "    'Cause': [\n",
        "        'Out of memory (OOM)',\n",
        "        'Exceeded time limit',\n",
        "        'Package not in container',\n",
        "        'Path not bound in Singularity',\n",
        "        'Resources not available'\n",
        "    ],\n",
        "    'Solution': [\n",
        "        'Increase --mem or optimize code',\n",
        "        'Increase --time or parallelize more',\n",
        "        'Rebuild container with package',\n",
        "        'Add --bind /path to singularity exec',\n",
        "        'Try debug partition or reduce resources'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"Troubleshooting Guide\")\n",
        "print(\"=\" * 90)\n",
        "print(troubleshooting.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsFUWUUfWGVV"
      },
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "1. **SLURM** schedules jobs on HPC clusters\n",
        "   - `sbatch` for batch jobs\n",
        "   - `srun` for interactive commands\n",
        "   - `squeue` to monitor jobs\n",
        "\n",
        "2. **Partitions** have different resource limits\n",
        "   - `debug`: Fast queue, limited time\n",
        "   - `shared`: Up to 128 cores, 256 GB\n",
        "   - `compute`: Full nodes, large jobs\n",
        "\n",
        "3. **Spark configuration** must match allocation\n",
        "   - Driver memory: ~10-15% of total\n",
        "   - Executor memory: ~75-80% of total\n",
        "   - Partitions: 2-3x number of cores\n",
        "\n",
        "4. **Singularity** provides reproducible environments\n",
        "   - Build locally, run on cluster\n",
        "   - Bind paths for filesystem access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z9OGww6WGVV",
        "outputId": "34a4a9b6-34ea-4481-c751-f85caeea01f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkSession stopped.\n"
          ]
        }
      ],
      "source": [
        "# Cleanup\n",
        "spark.stop()\n",
        "print(\"SparkSession stopped.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTtqx3SGWGVV"
      },
      "source": [
        "---\n",
        "\n",
        "## Next: Module 3 - Introduction to Ray\n",
        "\n",
        "In the next module, we'll introduce Ray.io and learn:\n",
        "- Ray Core: Tasks and Actors\n",
        "- Ray Data: Distributed datasets\n",
        "- Ray Train: Distributed training\n",
        "\n",
        "See: `03_ray_core.ipynb`"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}